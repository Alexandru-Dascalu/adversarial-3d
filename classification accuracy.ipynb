{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9405efd-e82c-4765-b7fb-a68e7b467f6f",
   "metadata": {},
   "source": [
    "## Evaluating adversarial textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6901eac2-1a9d-4b40-8a1f-d6ae5973c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import data\n",
    "import diff_rendering\n",
    "import renderer\n",
    "from config import cfg\n",
    "from imagenet_labels import imagenet_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a21f37-97d2-490e-9f8a-b7768e60ca42",
   "metadata": {},
   "source": [
    "Methods for rendering images used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8b2234-f0a6-4a00-adc5-67f2ea27bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_images_for_texture(std_texture, adv_texture, uv_renderer, model_name, target_label):\n",
    "    std_images = []\n",
    "    adv_images = []\n",
    "    for i in range(100):\n",
    "        std_image, adv_image = render_image_for_texture(std_texture, adv_texture, uv_renderer)\n",
    "        std_images.append(std_image)\n",
    "        adv_images.append(adv_image)\n",
    "\n",
    "        save_rendered_images(std_image, adv_image, model_name, target_label, i)\n",
    "\n",
    "    # convert list of numpy images to one single numpy array\n",
    "    std_images = np.stack(std_images, axis=0)\n",
    "    adv_images = np.stack(adv_images, axis=0)\n",
    "    # scale images from 0 to 1 values to values between -1 and 1, as that is what neural networks expect\n",
    "    std_images = 2 * std_images - 1\n",
    "    adv_images = 2 * adv_images - 1\n",
    "\n",
    "    return std_images, adv_images\n",
    "\n",
    "\n",
    "def render_image_for_texture(std_texture, adv_texture, renderer):\n",
    "    width = std_texture.shape[1]\n",
    "    height = std_texture.shape[0]\n",
    "\n",
    "    uv_map = renderer.render(1)\n",
    "    uv_map = uv_map * np.asarray([width - 1, height - 1], dtype=np.float32)\n",
    "\n",
    "    std_image, adv_image = diff_rendering.render(std_texture, adv_texture, uv_map)\n",
    "\n",
    "    # convert tensors to numpy arrays and discard the batch dimension\n",
    "    std_image = std_image.numpy()[0]\n",
    "    adv_image = adv_image.numpy()[0]\n",
    "\n",
    "    return std_image, adv_image\n",
    "\n",
    "\n",
    "def save_rendered_images(std_image, adv_image, model_name, target_label, num_image):\n",
    "    if not os.path.exists('./evaluation_images/normal/{}'.format(model_name)):\n",
    "        os.makedirs('./evaluation_images/normal/{}'.format(model_name))\n",
    "\n",
    "    if not os.path.exists('./evaluation_images/normal/{}/{}'.format(model_name, target_label)):\n",
    "        os.makedirs('./evaluation_images/normal/{}/{}'.format(model_name, target_label))\n",
    "\n",
    "    if not os.path.exists('./evaluation_images/adv/{}'.format(model_name)):\n",
    "        os.makedirs('./evaluation_images/adv/{}'.format(model_name))\n",
    "\n",
    "    if not os.path.exists('./evaluation_images/adv/{}/{}'.format(model_name, target_label)):\n",
    "        os.makedirs('./evaluation_images/adv/{}/{}'.format(model_name, target_label))\n",
    "\n",
    "    # Pillow only accepts numpy arrays with integer values as valid images\n",
    "    std_image = (std_image * 255).astype('uint8')\n",
    "    adv_image = (adv_image * 255).astype('uint8')\n",
    "\n",
    "    Image.fromarray(std_image, 'RGB').save('./evaluation_images/normal/{}/{}/image_{}.jpg'.format(\n",
    "        model_name, target_label, num_image))\n",
    "    Image.fromarray(adv_image, 'RGB').save('./evaluation_images/adv/{}/{}/image_{}.jpg'.format(\n",
    "        model_name, target_label, num_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e70e88-c0b3-416f-b557-c246d941e423",
   "metadata": {},
   "source": [
    "Methods for calculating TFR and accuracy based on the logits from the victim model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4dbee8-9cce-4e24-8d05-7616a9e4207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prediction_true(true_labels, predicted_label):\n",
    "    if true_labels == \"dog\":\n",
    "        # dog model has all 120 dog breed and dog-like animals as true labels\n",
    "        if 150 < predicted_label < 276:\n",
    "            return True\n",
    "    # even if object only has one true label, it is still represented as a list with just one element\n",
    "    elif type(true_labels) == list:\n",
    "        if predicted_label in true_labels:\n",
    "            return True\n",
    "    else:\n",
    "        raise ValueError(\"true labels list for a sample should be either \\\"dog\\\" or a list of ints.\")\n",
    "\n",
    "    # if it has not returned so far, then the prediction is incorrect\n",
    "    return False\n",
    "\n",
    "def get_tfr_and_accuracy(model, target_label, predictions):\n",
    "    label_predictions = [np.argmax(prediction) for prediction in predictions]\n",
    "    \n",
    "    accuracy = sum([is_prediction_true(model.labels, predicted_label) for predicted_label in label_predictions])\n",
    "    accuracy = accuracy / len(label_predictions)\n",
    "\n",
    "    tfr = sum([target_label == predicted_label for predicted_label in label_predictions])\n",
    "    tfr  = tfr / len(label_predictions)\n",
    "\n",
    "    return accuracy, tfr\n",
    "\n",
    "\n",
    "def save_result(result_dict, result, model_name, target_label, tfr):\n",
    "    \"\"\"\n",
    "    Save tfr or accuracy evaluation result to a dictionary based on the model and target label of the texture that was\n",
    "    evaluated.\n",
    "    \"\"\"\n",
    "    # initialise sub-dictionary for that particular model\n",
    "    if model_name not in result_dict:\n",
    "        result_dict[model_name] = dict()\n",
    "\n",
    "    # initialise sub-dictionary for that particular target label in the sub-dictionary for the given model\n",
    "    if target_label not in result_dict[model_name]:\n",
    "        result_dict[model_name][target_label] = dict()\n",
    "\n",
    "    # we may want to either save the TFR of adversarial texture or its accuracy\n",
    "    if tfr:\n",
    "        result_dict[model_name][target_label]['tfr'] = result\n",
    "    else:\n",
    "        result_dict[model_name][target_label]['accuracy'] = result\n",
    "        \n",
    "def save_num_steps(num_steps_dict, num_steps, model_name, target_label):\n",
    "    \"\"\"\n",
    "    Save tfr or accuracy evaluation result to a dictionary based on the model and target label of the texture that was\n",
    "    evaluated.\n",
    "    \"\"\"\n",
    "    # initialise sub-dictionary for that particular model\n",
    "    if model_name not in num_steps_dict:\n",
    "        num_steps_dict[model_name] = dict()\n",
    "\n",
    "    # initialise sub-dictionary for that particular target label in the sub-dictionary for the given model\n",
    "    if target_label not in num_steps_dict[model_name]:\n",
    "        num_steps_dict[model_name][target_label] = dict()\n",
    "\n",
    "    num_steps_dict[model_name][target_label] = num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f762762-fee1-4d35-8520-beafae5c9a67",
   "metadata": {},
   "source": [
    "Methods for calculating averages of results saved in the dictionaries for normal and adversarial images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659a5573-b04c-4716-9d54-451fbc38d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_metric(results_dict, for_tfr):\n",
    "    metric_sum = 0\n",
    "    metric_count = 0\n",
    "\n",
    "    for model_name in results_dict:\n",
    "        metric_sum += get_average_metric_for_model(results_dict, model_name, for_tfr)\n",
    "        metric_count += 1\n",
    "\n",
    "    average = metric_sum / metric_count\n",
    "    return average\n",
    "\n",
    "\n",
    "def get_average_metric_for_model(results_dict, model_name, for_tfr):\n",
    "    metric_sum = 0\n",
    "    metric_count = 0\n",
    "\n",
    "    for target_label in results_dict[model_name]:\n",
    "        if for_tfr:\n",
    "            metric_sum += results_dict[model_name][target_label]['tfr']\n",
    "        else:\n",
    "            metric_sum += results_dict[model_name][target_label]['accuracy']\n",
    "        metric_count += 1\n",
    "\n",
    "    average = metric_sum / metric_count\n",
    "    return average\n",
    "\n",
    "def get_average_num_steps(num_steps_dict, model_name):\n",
    "    num_steps_sum = 0\n",
    "    num_steps_count = 0\n",
    "\n",
    "    for target_label in num_steps_dict[model_name]:\n",
    "        num_steps_sum += num_steps_dict[model_name][target_label]\n",
    "        num_steps_count += 1\n",
    "\n",
    "    average = num_steps_sum / num_steps_count\n",
    "    return average\n",
    "\n",
    "def flatten_dict(result_dict, for_tfr):\n",
    "    result_list = []\n",
    "    for model in result_dict:\n",
    "        for target_label in result_dict[model]:\n",
    "            if for_tfr:\n",
    "                result_list.append(result_dict[model][target_label]['tfr'])\n",
    "            else:\n",
    "                result_list.append(result_dict[model][target_label]['accuracy'])\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e47fc-1387-489d-b9a9-46d9575a0529",
   "metadata": {},
   "source": [
    "Methods for parsing file names of adversarial textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03826695-a9bf-40a7-bd5d-b8bae8ebf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_adv_texture_file_name(file_name):\n",
    "    \"\"\"\n",
    "    Extracts useful information from the name of the files where the adversarial examples were saved.\n",
    "    \"\"\"\n",
    "    # removed extension from image file name\n",
    "    file_name, _ = os.path.splitext(file_name)\n",
    "    file_name_split = file_name.split('_')\n",
    "\n",
    "    target_label = int(file_name_split[-3])\n",
    "    num_steps = int(file_name_split[-1])\n",
    "\n",
    "    index_first_digit = get_index_first_digit(file_name)\n",
    "    # before the first digit in the file name, there is an underscore, which is not part of the name. Therefore we\n",
    "    # slice until index_first_digit - 1\n",
    "    model_name = file_name[:(index_first_digit - 1)]\n",
    "\n",
    "    return model_name, target_label, num_steps\n",
    "\n",
    "\n",
    "def get_index_first_digit(string):\n",
    "    \"\"\"\n",
    "    Returns index of the first digit to be found in a string.\n",
    "    \"\"\"\n",
    "    for i, character in enumerate(string):\n",
    "        if character.isdigit():\n",
    "            return i\n",
    "    raise ValueError(\"The given string is expectde to have numbers in it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380b8270-9ea4-44b2-b8f4-d7cdafd0ae69",
   "metadata": {},
   "source": [
    "Creating necessary objects for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6118173-508a-429d-95dc-b8c7c653b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barrel: labels [427]\n",
      "baseball: labels [429]\n",
      "camaro: labels [817, 436, 751]\n",
      "clownfish: labels [393]\n",
      "crocodile: labels [49, 50]\n",
      "german_shepherd: labels dog\n",
      "jeep: labels [609, 586, 408]\n",
      "orange: labels [950]\n",
      "orca: labels [148]\n",
      "purse: labels [748, 893]\n",
      "rugby_ball: labels [768]\n",
      "running_shoe: labels [770]\n",
      "sea_turtle: labels [33, 34, 35, 36, 37]\n",
      "taxi: labels [468]\n",
      "teddy: labels [850]\n"
     ]
    }
   ],
   "source": [
    "models = data.load_dataset(\"./dataset\")\n",
    "\n",
    "# make renderer used for creating UV maps\n",
    "uv_renderer = renderer.Renderer((299, 299))\n",
    "uv_renderer.set_parameters(\n",
    "    camera_distance=(cfg.camera_distance_min, cfg.camera_distance_max),\n",
    "    x_translation=(cfg.x_translation_min, cfg.x_translation_max),\n",
    "    y_translation=(cfg.y_translation_min, cfg.y_translation_max)\n",
    ")\n",
    "\n",
    "victim_model = tf.keras.applications.inception_v3.InceptionV3(\n",
    "    include_top=True,\n",
    "    weights='imagenet',\n",
    "    classes=1000,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "victim_model.compile(optimizer='adam',\n",
    "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb45fd6-4f88-4e41-99f6-3ea05a4e2de3",
   "metadata": {},
   "source": [
    "For every adversarial texture, render 100 evaluation images of that model with the adversarial texture, together with 100 images using the normal texture of that model, but using the exact same positions and backgrounds as the adversarial images. Then use the victim model to evaluate how often are these images labelled with a correct label vs the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c400556-dc4f-4c43-a2a8-209e4b97a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating evaluation renders for model crocodile, target label 49 (African crocodile, Nile crocodile, Crocodylus niloticus)\n",
      "100/100 [==============================] - 5s 46ms/step\n",
      "Evaluating normal images: TFR: 0.14, Accuracy: 0.44\n",
      "100/100 [==============================] - 4s 44ms/step\n",
      "Evaluating adversarial images: TFR: 0.14, Accuracy: 0.44\n",
      "\n",
      "Creating evaluation renders for model jeep, target label 609 (jeep, landrover)\n",
      "100/100 [==============================] - 4s 44ms/step\n",
      "Evaluating normal images: TFR: 0.16, Accuracy: 0.19\n",
      "100/100 [==============================] - 4s 41ms/step\n",
      "Evaluating adversarial images: TFR: 0.16, Accuracy: 0.19\n",
      "\n",
      "Creating evaluation renders for model orca, target label 148 (killer whale, killer, orca, grampus, sea wolf, Orcinus orca)\n",
      "100/100 [==============================] - 4s 39ms/step\n",
      "Evaluating normal images: TFR: 0.69, Accuracy: 0.69\n",
      "100/100 [==============================] - 4s 42ms/step\n",
      "Evaluating adversarial images: TFR: 0.69, Accuracy: 0.69\n",
      "\n",
      "Creating evaluation renders for model rugby_ball, target label 768 (rugby ball)\n",
      "100/100 [==============================] - 4s 42ms/step\n",
      "Evaluating normal images: TFR: 0.95, Accuracy: 0.95\n",
      "100/100 [==============================] - 4s 41ms/step\n",
      "Evaluating adversarial images: TFR: 0.95, Accuracy: 0.95\n",
      "\n",
      "Creating evaluation renders for model running_shoe, target label 770 (running shoe)\n",
      "100/100 [==============================] - 4s 42ms/step\n",
      "Evaluating normal images: TFR: 0.35, Accuracy: 0.35\n",
      "100/100 [==============================] - 4s 42ms/step\n",
      "Evaluating adversarial images: TFR: 0.35, Accuracy: 0.35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dictionaries used to record results of the evaluation. Each dict has sub-dictionaries for each model and\n",
    "# target label\n",
    "normal_results = dict()\n",
    "adv_results = dict()\n",
    "num_steps_dict = dict()\n",
    "\n",
    "for image_file_name in os.listdir(\"./adv_textures\"):\n",
    "    adv_texture = data.Model3D._get_texture(\"./adv_textures/{}\".format(image_file_name))\n",
    "\n",
    "    # extract information from file name of adversarial texture, inclduing which model and target label is the\n",
    "    # texture for\n",
    "    current_model_name, current_target_label, num_steps = parse_adv_texture_file_name(image_file_name)\n",
    "    save_num_steps(num_steps_dict, num_steps, current_model_name, current_target_label)\n",
    "\n",
    "    # find the model that the adversarial texture was made for\n",
    "    current_model = next(x for x in models if x.name == current_model_name)\n",
    "    # get the normal texture of the model\n",
    "    std_texture = current_model.raw_texture\n",
    "    # load the appropriate model into the renderer\n",
    "    uv_renderer.load_obj(current_model.obj_path)\n",
    "\n",
    "    print(\"Creating evaluation renders for model {}, target label {} ({})\".format(\n",
    "        current_model_name, current_target_label, imagenet_labels[current_target_label]))\n",
    "    std_images, adv_images = render_images_for_texture(std_texture, adv_texture, uv_renderer, current_model_name,\n",
    "                                                       current_target_label)\n",
    "\n",
    "    # evaluate renders with the normal image\n",
    "    predictions = victim_model.predict(std_images, batch_size=1)\n",
    "    accuracy, tfr = get_tfr_and_accuracy(current_model, current_target_label, predictions)\n",
    "    # record results in dictionary and on the command line\n",
    "    save_result(normal_results, tfr, current_model_name, current_target_label, tfr=True)\n",
    "    save_result(normal_results, accuracy, current_model_name, current_target_label, tfr=False)\n",
    "    print(\"Evaluating normal images: TFR: {}, Accuracy: {}\".format(tfr, accuracy))\n",
    "\n",
    "    # evaluate renders with the adversarial image\n",
    "    predictions = victim_model.predict(adv_images, batch_size=1)\n",
    "    accuracy, tfr = get_tfr_and_accuracy(current_model, current_target_label, predictions)\n",
    "    # record results in dictionary and on the command line\n",
    "    save_result(adv_results, tfr, current_model_name, current_target_label, tfr=True)\n",
    "    save_result(adv_results, accuracy, current_model_name, current_target_label, tfr=False)\n",
    "    print(\"Evaluating adversarial images: TFR: {}, Accuracy: {}\\n\".format(tfr, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9114eaa8-97d6-483b-b5f3-7d22a01c7c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy for normal images: 0.6028\n",
      "Average TFR for normal images: 0.0008\n",
      "Average accuracy for adversarial images: 0.0086\n",
      "Average TFR for adversarial images: 0.8008000000000001\n",
      "\n",
      "\n",
      "Average TFR on adversarial images for model barrel: 0.9559999999999998\n",
      "Average classification accuracy on normal images for model barrel: 0.9280000000000002\n",
      "Average iterations for creating adversarial examples for model barrel: 1428.4\n",
      "\n",
      "Average TFR on adversarial images for model baseball: 0.806\n",
      "Average classification accuracy on normal images for model baseball: 1.0\n",
      "Average iterations for creating adversarial examples for model baseball: 6349.2\n",
      "\n",
      "Average TFR on adversarial images for model camaro: 0.9339999999999999\n",
      "Average classification accuracy on normal images for model camaro: 0.10800000000000001\n",
      "Average iterations for creating adversarial examples for model camaro: 2585.8\n",
      "\n",
      "Average TFR on adversarial images for model clownfish: 0.48200000000000004\n",
      "Average classification accuracy on normal images for model clownfish: 0.372\n",
      "Average iterations for creating adversarial examples for model clownfish: 9999.0\n",
      "\n",
      "Average TFR on adversarial images for model german_shepherd: 0.8560000000000001\n",
      "Average classification accuracy on normal images for model german_shepherd: 0.732\n",
      "Average iterations for creating adversarial examples for model german_shepherd: 5243.0\n",
      "\n",
      "Average TFR on adversarial images for model orange: 0.974\n",
      "Average classification accuracy on normal images for model orange: 0.59\n",
      "Average iterations for creating adversarial examples for model orange: 1163.2\n",
      "\n",
      "Average TFR on adversarial images for model purse: 0.42800000000000005\n",
      "Average classification accuracy on normal images for model purse: 0.7100000000000001\n",
      "Average iterations for creating adversarial examples for model purse: 9090.6\n",
      "\n",
      "Average TFR on adversarial images for model sea_turtle: 0.8039999999999999\n",
      "Average classification accuracy on normal images for model sea_turtle: 0.898\n",
      "Average iterations for creating adversarial examples for model sea_turtle: 5123.0\n",
      "\n",
      "Average TFR on adversarial images for model taxi: 0.8780000000000001\n",
      "Average classification accuracy on normal images for model taxi: 0.15\n",
      "Average iterations for creating adversarial examples for model taxi: 4229.6\n",
      "\n",
      "Average TFR on adversarial images for model teddy: 0.89\n",
      "Average classification accuracy on normal images for model teddy: 0.54\n",
      "Average iterations for creating adversarial examples for model teddy: 2782.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy for normal images: {}\".format(get_average_metric(normal_results, for_tfr=False)))\n",
    "print(\"Average TFR for normal images: {}\".format(get_average_metric(normal_results, for_tfr=True)))\n",
    "\n",
    "print(\"Average accuracy for adversarial images: {}\".format(get_average_metric(adv_results, for_tfr=False)))\n",
    "print(\"Average TFR for adversarial images: {}\\n\".format(get_average_metric(adv_results, for_tfr=True)))\n",
    "\n",
    "for model_name in adv_results:\n",
    "    print(\"\\nAverage TFR on adversarial images for model {}: {}\".format(model_name, get_average_metric_for_model(\n",
    "        adv_results, model_name, for_tfr=True)))\n",
    "    print(\"Average classification accuracy on normal images for model {}: {}\".format(\n",
    "        model_name, get_average_metric_for_model(normal_results, model_name, for_tfr=False)))\n",
    "    print(\"Average iterations for creating adversarial examples for model {}: {}\".format(\n",
    "        model_name, get_average_num_steps(num_steps_dict, model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962075c4-d285-41ae-aefc-46d2e69544f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
