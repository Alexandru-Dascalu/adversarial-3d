{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9405efd-e82c-4765-b7fb-a68e7b467f6f",
   "metadata": {},
   "source": [
    "## Evaluating classification accuracy for extra models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd54a42-cac2-4cc6-9fa4-4648d34c8103",
   "metadata": {},
   "source": [
    "This notebook shows the results of the experiment where the classification accuracy was measured for images of five 3D models: crocodile, rugby ball, running shoe, orca and jeep. These measurements are done separately from evaluation.ipynb because these models were not used in the experiment where EOT was used to create adversarial textures, and therefore unlike the other 10 models, there are no adversarial textures to evaluate alongside the normal texture. The classification accuracies of images with the normal textures of the other 10 models are found in experiments.ipynb ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6901eac2-1a9d-4b40-8a1f-d6ae5973c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import data\n",
    "import diff_rendering\n",
    "import renderer\n",
    "from config import cfg\n",
    "from imagenet_labels import imagenet_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a21f37-97d2-490e-9f8a-b7768e60ca42",
   "metadata": {},
   "source": [
    "Methods for rendering images used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8b2234-f0a6-4a00-adc5-67f2ea27bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_images_for_texture(std_texture, adv_texture, uv_renderer, model_name, target_label):\n",
    "    \"\"\"\n",
    "    Renders evaluation images for a certain 3D model and target label.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    std_texture : numpy array\n",
    "        A numpy array with shape [image_height, image_width, 3]. Represents the normal texture of the 3D model.\n",
    "    adv_texture : numpy array\n",
    "        A numpy array with shape [image_height, image_width, 3]. Represents an adversarial texture of the 3D model.\n",
    "    uv_renderer : Renderer\n",
    "        The renderer used to create the UV maps used to create the images.\n",
    "    model_name : str\n",
    "        The name of the 3D model. Only matters for saving the rendered images to a file.\n",
    "    target_label : int\n",
    "        Target label for which the given adversarial texture was made. Only matters for saving the rendered images to a\n",
    "        file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Two tensors. The first one is of shape 100 x 299 x 299 x 3, representing the images of the new\n",
    "        renders with the normal texture. The second is of shape 100 x 299 x 299 x 3, representing the\n",
    "        images of the new renders with the adversarial texture.\n",
    "    \"\"\"\n",
    "    std_images = []\n",
    "    adv_images = []\n",
    "    for i in range(100):\n",
    "        std_image, adv_image = render_image_for_texture(std_texture, adv_texture, uv_renderer)\n",
    "        std_images.append(std_image)\n",
    "        adv_images.append(adv_image)\n",
    "\n",
    "        save_rendered_images(std_image, adv_image, model_name, target_label, i)\n",
    "\n",
    "    # convert list of numpy images to one single numpy array\n",
    "    std_images = np.stack(std_images, axis=0)\n",
    "    adv_images = np.stack(adv_images, axis=0)\n",
    "    # scale images from 0 to 1 values to values between -1 and 1, as that is what neural networks expect\n",
    "    std_images = 2 * std_images - 1\n",
    "    adv_images = 2 * adv_images - 1\n",
    "\n",
    "    return std_images, adv_images\n",
    "\n",
    "\n",
    "def render_image_for_texture(std_texture, adv_texture, renderer):\n",
    "    \"\"\"\n",
    "    Renders a pair of two evaluation images for a certain 3D model and target label. One image uses the normal texture,\n",
    "    while the other has the exact same pose of the 3D object, background colour, and other params, but uses the\n",
    "    adversarial texture instead.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    std_texture : numpy array\n",
    "        A numpy array with shape [image_height, image_width, 3]. Represents the normal texture of the 3D model.\n",
    "    adv_texture : numpy array\n",
    "        A numpy array with shape [image_height, image_width, 3]. Represents an adversarial texture of the 3D model.\n",
    "    uv_renderer : Renderer\n",
    "        The renderer used to create the UV maps used to create the images.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Two tensors. The first one is of shape 299 x 299 x 3, representing the image of the new renders with the normal\n",
    "        texture. The second is of shape 299 x 299 x 3, representing the images of the new renders with the adversarial\n",
    "        texture.\n",
    "    \"\"\"\n",
    "    width = std_texture.shape[1]\n",
    "    height = std_texture.shape[0]\n",
    "\n",
    "    uv_map = renderer.render(1)\n",
    "    uv_map = uv_map * np.asarray([width - 1, height - 1], dtype=np.float32)\n",
    "\n",
    "    std_image, adv_image = diff_rendering.render(std_texture, adv_texture, uv_map)\n",
    "\n",
    "    # convert tensors to numpy arrays and discard the batch dimension\n",
    "    std_image = std_image.numpy()[0]\n",
    "    adv_image = adv_image.numpy()[0]\n",
    "\n",
    "    return std_image, adv_image\n",
    "\n",
    "\n",
    "def save_rendered_images(std_image, adv_image, model_name, target_label, num_image):\n",
    "    \"\"\"\n",
    "    Save a pair of evaluation images to the file system.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    std_image : numpy array\n",
    "        A numpy array with shape [image_height, image_width, 3]. Represents an image with the normal texture.\n",
    "    adv_image : numpy array\n",
    "        A numpy array with shape [image_height, image_width, 3]. Represents an image with the adversarial texture.\n",
    "    model_name : str\n",
    "        The name of the 3D model rendered in these images.\n",
    "    target_label : int\n",
    "        Target label for which the adversarial texture in adv_image was made.\n",
    "    num_image : int\n",
    "        The index of this pair of renders. Used to give the file name a unique name.\n",
    "    \"\"\"\n",
    "    if not os.path.exists('./evaluation_images/normal/{}'.format(model_name)):\n",
    "        os.makedirs('./evaluation_images/normal/{}'.format(model_name))\n",
    "\n",
    "    if not os.path.exists('./evaluation_images/normal/{}/{}'.format(model_name, target_label)):\n",
    "        os.makedirs('./evaluation_images/normal/{}/{}'.format(model_name, target_label))\n",
    "\n",
    "    if not os.path.exists('./evaluation_images/adv/{}'.format(model_name)):\n",
    "        os.makedirs('./evaluation_images/adv/{}'.format(model_name))\n",
    "\n",
    "    if not os.path.exists('./evaluation_images/adv/{}/{}'.format(model_name, target_label)):\n",
    "        os.makedirs('./evaluation_images/adv/{}/{}'.format(model_name, target_label))\n",
    "\n",
    "    # Pillow only accepts numpy arrays with integer values as valid images\n",
    "    std_image = (std_image * 255).astype('uint8')\n",
    "    adv_image = (adv_image * 255).astype('uint8')\n",
    "\n",
    "    Image.fromarray(std_image, 'RGB').save('./evaluation_images/normal/{}/{}/image_{}.jpg'.format(\n",
    "        model_name, target_label, num_image))\n",
    "    Image.fromarray(adv_image, 'RGB').save('./evaluation_images/adv/{}/{}/image_{}.jpg'.format(\n",
    "        model_name, target_label, num_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e70e88-c0b3-416f-b557-c246d941e423",
   "metadata": {},
   "source": [
    "Methods for calculating TFR and accuracy based on the logits from the victim model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4dbee8-9cce-4e24-8d05-7616a9e4207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prediction_true(true_labels, predicted_label):\n",
    "    \"\"\"\n",
    "    Check if predicted label is a ground truth label.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_labels : list\n",
    "        The list of ground truth labels for a 3D model.\n",
    "    predicted_label : int\n",
    "        The predicted label.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the prediction is correct, false if not.\n",
    "    \"\"\"\n",
    "    if true_labels == \"dog\":\n",
    "        # dog model has all 120 dog breed and dog-like animals as true labels\n",
    "        if 150 < predicted_label < 276:\n",
    "            return True\n",
    "    # even if object only has one true label, it is still represented as a list with just one element\n",
    "    elif type(true_labels) == list:\n",
    "        if predicted_label in true_labels:\n",
    "            return True\n",
    "    else:\n",
    "        raise ValueError(\"true labels list for a sample should be either \\\"dog\\\" or a list of ints.\")\n",
    "\n",
    "    # if it has not returned so far, then the prediction is incorrect\n",
    "    return False\n",
    "\n",
    "def get_tfr_and_accuracy(model, target_label, predictions):\n",
    "    \"\"\"\n",
    "    Calculate TFR and classification accuracy for a set of predictions made by a neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : data.Model3D\n",
    "        The 3D model in the rendered images for which predictions were made.\n",
    "    target_label : int\n",
    "        Adversarial target label that we want to measure TFR for.\n",
    "    predictions : numpy array\n",
    "        The raw logits that the neural network outputed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple with two values. The first is the accuracy, the second is the TFR.\n",
    "    \"\"\"\n",
    "    label_predictions = [np.argmax(prediction) for prediction in predictions]\n",
    "    \n",
    "    accuracy = sum([is_prediction_true(model.labels, predicted_label) for predicted_label in label_predictions])\n",
    "    accuracy = accuracy / len(label_predictions)\n",
    "\n",
    "    tfr = sum([target_label == predicted_label for predicted_label in label_predictions])\n",
    "    tfr  = tfr / len(label_predictions)\n",
    "\n",
    "    return accuracy, tfr\n",
    "\n",
    "\n",
    "def save_result(result_dict, result, model_name, target_label, tfr):\n",
    "    \"\"\"\n",
    "    Save tfr or accuracy evaluation result to a dictionary based on the model and target label of the texture that was\n",
    "    evaluated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_dict : dict\n",
    "        Nested dictionary holding experiment results. It has an entry for each 3D model, mapping the model name to\n",
    "        another dictionary. This dictionary has an entry for each target label for which an adversarial texture was\n",
    "        made. The key is the int target label, and the value is another dictionary. This dictionary has two entries,\n",
    "        one for the tfr and the other for the classification accuracy. The key must be 'tfr'/'accuracy' and the value\n",
    "        is a float number between 0 and 1.\n",
    "    result : float\n",
    "        The value of a measurement metric we want to save in the dictionary. The metric is either is for the TFR or for\n",
    "        the classification accuracy\n",
    "    model_name : str\n",
    "        The name of the 3D model for the given result was recorded.\n",
    "    target_label : int\n",
    "        The target label for which the given result was recorded.\n",
    "    tfr : bool\n",
    "        Whether the given result is the TFR or not.\n",
    "    \"\"\"\n",
    "    # initialise sub-dictionary for that particular model\n",
    "    if model_name not in result_dict:\n",
    "        result_dict[model_name] = dict()\n",
    "\n",
    "    # initialise sub-dictionary for that particular target label in the sub-dictionary for the given model\n",
    "    if target_label not in result_dict[model_name]:\n",
    "        result_dict[model_name][target_label] = dict()\n",
    "\n",
    "    # we may want to either save the TFR of adversarial texture or its accuracy\n",
    "    if tfr:\n",
    "        result_dict[model_name][target_label]['tfr'] = result\n",
    "    else:\n",
    "        result_dict[model_name][target_label]['accuracy'] = result\n",
    "        \n",
    "def save_num_steps(num_steps_dict, num_steps, model_name, target_label):\n",
    "    \"\"\"\n",
    "    Save number of EOT optimisation steps used to create an adversarial texture. It is saved to a dictionary based on\n",
    "    the model and target label of the texture.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_steps_dict : dict\n",
    "        Nested dictionary holding experiment results. It has an entry for each 3D model, mapping the model name to\n",
    "        another dictionary. This dictionary has an entry for each target label for which an adversarial texture was\n",
    "        made. The key is the int target label, and the value is the number of steps for the texture of that 3D model\n",
    "        and that target label.\n",
    "    num_steps : int\n",
    "        The number of EOT optimisation steps that took to create that adversarial texture.\n",
    "    model_name : str\n",
    "        The name of the 3D model for the given result was recorded.\n",
    "    target_label : int\n",
    "        The target label for which the given result was recorded.\n",
    "    \"\"\"\n",
    "    # initialise sub-dictionary for that particular model\n",
    "    if model_name not in num_steps_dict:\n",
    "        num_steps_dict[model_name] = dict()\n",
    "\n",
    "    # initialise sub-dictionary for that particular target label in the sub-dictionary for the given model\n",
    "    if target_label not in num_steps_dict[model_name]:\n",
    "        num_steps_dict[model_name][target_label] = dict()\n",
    "\n",
    "    num_steps_dict[model_name][target_label] = num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f762762-fee1-4d35-8520-beafae5c9a67",
   "metadata": {},
   "source": [
    "Methods for calculating averages of results saved in the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659a5573-b04c-4716-9d54-451fbc38d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_metric(results_dict, for_tfr):\n",
    "    \"\"\"\n",
    "    Calculate average of the TFR/classification accuracy across all 3D models.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_dict : dict\n",
    "        Nested dictionary holding experiment results. It has an entry for each 3D model, mapping the model name to\n",
    "        another dictionary. This dictionary has an entry for each target label for which an adversarial texture was\n",
    "        made. The key is the int target label, and the value is another dictionary. This dictionary has two entries,\n",
    "        one for the tfr and the other for the classification accuracy. The key must be 'tfr'/'accuracy' and the value\n",
    "        is a float number between 0 and 1.\n",
    "    for_tfr : bool\n",
    "        Whether we calculate the mean of the TFR, or of the classification accuracy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean TFR or classification accuracy across all 3D models for which there are results in results_dict.\n",
    "    \"\"\"\n",
    "    metric_sum = 0\n",
    "    metric_count = 0\n",
    "\n",
    "    for model_name in results_dict:\n",
    "        metric_sum += get_average_metric_for_model(results_dict, model_name, for_tfr)\n",
    "        metric_count += 1\n",
    "\n",
    "    average = metric_sum / metric_count\n",
    "    return average\n",
    "\n",
    "\n",
    "def get_average_metric_for_model(results_dict, model_name, for_tfr):\n",
    "    \"\"\"\n",
    "    Calculate average of the TFR/classification accuracy for all adversarial textures of the given 3D model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_dict : dict\n",
    "        Nested dictionary holding experiment results. It has an entry for each 3D model, mapping the model name to\n",
    "        another dictionary. This dictionary has an entry for each target label for which an adversarial texture was\n",
    "        made. The key is the int target label, and the value is another dictionary. This dictionary has two entries,\n",
    "        one for the tfr and the other for the classification accuracy. The key must be 'tfr'/'accuracy' and the value\n",
    "        is a float number between 0 and 1.\n",
    "    model_name : str\n",
    "        The name of the 3D model for which the mean is calculated.\n",
    "    for_tfr : bool\n",
    "        Whether we calculate the mean of the TFR, or of the classification accuracy.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean TFR or classification accuracy for one 3D model for which there are results in results_dict.\n",
    "    \"\"\"\n",
    "    metric_sum = 0\n",
    "    metric_count = 0\n",
    "\n",
    "    for target_label in results_dict[model_name]:\n",
    "        if for_tfr:\n",
    "            metric_sum += results_dict[model_name][target_label]['tfr']\n",
    "        else:\n",
    "            metric_sum += results_dict[model_name][target_label]['accuracy']\n",
    "        metric_count += 1\n",
    "\n",
    "    average = metric_sum / metric_count\n",
    "    return average\n",
    "\n",
    "def get_average_num_steps(num_steps_dict, model_name):\n",
    "    \"\"\"\n",
    "    Calculate average of the TFR/classification accuracy for all adversarial textures of the given 3D model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_steps_dict : dict\n",
    "        Nested dictionary holding experiment results. It has an entry for each 3D model, mapping the model name to\n",
    "        another dictionary. This dictionary has an entry for each target label for which an adversarial texture was\n",
    "        made. The key is the int target label, and the value is the number of steps for the texture of that 3D model\n",
    "        and that target label.\n",
    "    model_name : str\n",
    "        The name of the 3D model for which the mean is calculated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean number of steps it took to create adversarial textures for the given model.\n",
    "    \"\"\"\n",
    "    num_steps_sum = 0\n",
    "    num_steps_count = 0\n",
    "\n",
    "    for target_label in num_steps_dict[model_name]:\n",
    "        num_steps_sum += num_steps_dict[model_name][target_label]\n",
    "        num_steps_count += 1\n",
    "\n",
    "    average = num_steps_sum / num_steps_count\n",
    "    return average\n",
    "\n",
    "def flatten_dict(result_dict, for_tfr):\n",
    "    \"\"\"\n",
    "    Serialises the the results dict into a list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    result_dict : dict\n",
    "        Nested dictionary holding experiment results. It has an entry for each 3D model, mapping the model name to\n",
    "        another dictionary. This dictionary has an entry for each target label for which an adversarial texture was\n",
    "        made. The key is the int target label, and the value is another dictionary. This dictionary has two entries,\n",
    "        one for the tfr and the other for the classification accuracy. The key must be 'tfr'/'accuracy' and the value\n",
    "        is a float number between 0 and 1.\n",
    "    for_tfr : bool\n",
    "        Whether the given result if the TFR or not.\n",
    "    \"\"\"\n",
    "    result_list = []\n",
    "    for model in result_dict:\n",
    "        for target_label in result_dict[model]:\n",
    "            if for_tfr:\n",
    "                result_list.append(result_dict[model][target_label]['tfr'])\n",
    "            else:\n",
    "                result_list.append(result_dict[model][target_label]['accuracy'])\n",
    "\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e47fc-1387-489d-b9a9-46d9575a0529",
   "metadata": {},
   "source": [
    "Methods for parsing file names of textures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03826695-a9bf-40a7-bd5d-b8bae8ebf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_adv_texture_file_name(file_name):\n",
    "    \"\"\"\n",
    "    Extracts useful information from the name of the files where the adversarial examples were saved.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        The name of the file.\n",
    "    \"\"\"\n",
    "    # removed extension from image file name\n",
    "    file_name, _ = os.path.splitext(file_name)\n",
    "    file_name_split = file_name.split('_')\n",
    "\n",
    "    target_label = int(file_name_split[-3])\n",
    "    num_steps = int(file_name_split[-1])\n",
    "\n",
    "    index_first_digit = get_index_first_digit(file_name)\n",
    "    # before the first digit in the file name, there is an underscore, which is not part of the name. Therefore we\n",
    "    # slice until index_first_digit - 1\n",
    "    model_name = file_name[:(index_first_digit - 1)]\n",
    "\n",
    "    return model_name, target_label, num_steps\n",
    "\n",
    "\n",
    "def get_index_first_digit(string):\n",
    "    \"\"\"\n",
    "    Returns index of the first digit to be found in a string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    string : str\n",
    "        The string to be searched.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Index where the first digit can be found.\n",
    "    \"\"\"\n",
    "    for i, character in enumerate(string):\n",
    "        if character.isdigit():\n",
    "            return i\n",
    "    raise ValueError(\"The given string is expectde to have numbers in it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380b8270-9ea4-44b2-b8f4-d7cdafd0ae69",
   "metadata": {},
   "source": [
    "Creating necessary objects for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6118173-508a-429d-95dc-b8c7c653b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barrel: labels [427]\n",
      "baseball: labels [429]\n",
      "camaro: labels [817, 436, 751]\n",
      "clownfish: labels [393]\n",
      "crocodile: labels [49, 50]\n",
      "german_shepherd: labels dog\n",
      "jeep: labels [609, 586, 408]\n",
      "orange: labels [950]\n",
      "orca: labels [148]\n",
      "purse: labels [748, 893]\n",
      "rugby_ball: labels [768]\n",
      "running_shoe: labels [770]\n",
      "sea_turtle: labels [33, 34, 35, 36, 37]\n",
      "taxi: labels [468]\n",
      "teddy: labels [850]\n"
     ]
    }
   ],
   "source": [
    "models = data.load_dataset(\"./dataset\")\n",
    "\n",
    "# make renderer used for creating UV maps\n",
    "uv_renderer = renderer.Renderer((299, 299))\n",
    "uv_renderer.set_parameters(\n",
    "    camera_distance=(cfg.camera_distance_min, cfg.camera_distance_max),\n",
    "    x_translation=(cfg.x_translation_min, cfg.x_translation_max),\n",
    "    y_translation=(cfg.y_translation_min, cfg.y_translation_max)\n",
    ")\n",
    "\n",
    "victim_model = tf.keras.applications.inception_v3.InceptionV3(\n",
    "    include_top=True,\n",
    "    weights='imagenet',\n",
    "    classes=1000,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "victim_model.compile(optimizer='adam',\n",
    "                     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb45fd6-4f88-4e41-99f6-3ea05a4e2de3",
   "metadata": {},
   "source": [
    "For every model, render 100 evaluation images of that model with the normal texture. Then use the victim model to evaluate how often are these images labelled with a correct label vs the target label. They are evaluate twice because this code use the rendering pipeline use for creating adversarial textures. This takes in a normal and an adversarial textures, and creates pairs of equivalent images where the only difference is the texture. Instead of refactoring that code, the normal textures is passed in an as an adversarial texture, and two identical sets of images with the normal texture are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c400556-dc4f-4c43-a2a8-209e4b97a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating evaluation renders for model crocodile, target label 49 (African crocodile, Nile crocodile, Crocodylus niloticus)\n",
      "100/100 [==============================] - 5s 46ms/step\n",
      "Evaluating normal images: TFR: 0.14, Accuracy: 0.44\n",
      "100/100 [==============================] - 4s 44ms/step\n",
      "Evaluating adversarial images: TFR: 0.14, Accuracy: 0.44\n",
      "\n",
      "Creating evaluation renders for model jeep, target label 609 (jeep, landrover)\n",
      "100/100 [==============================] - 4s 44ms/step\n",
      "Evaluating normal images: TFR: 0.16, Accuracy: 0.19\n",
      "100/100 [==============================] - 4s 41ms/step\n",
      "Evaluating adversarial images: TFR: 0.16, Accuracy: 0.19\n",
      "\n",
      "Creating evaluation renders for model orca, target label 148 (killer whale, killer, orca, grampus, sea wolf, Orcinus orca)\n",
      "100/100 [==============================] - 4s 39ms/step\n",
      "Evaluating normal images: TFR: 0.69, Accuracy: 0.69\n",
      "100/100 [==============================] - 4s 42ms/step\n",
      "Evaluating adversarial images: TFR: 0.69, Accuracy: 0.69\n",
      "\n",
      "Creating evaluation renders for model rugby_ball, target label 768 (rugby ball)\n",
      "100/100 [==============================] - 4s 42ms/step\n",
      "Evaluating normal images: TFR: 0.95, Accuracy: 0.95\n",
      "100/100 [==============================] - 4s 41ms/step\n",
      "Evaluating adversarial images: TFR: 0.95, Accuracy: 0.95\n",
      "\n",
      "Creating evaluation renders for model running_shoe, target label 770 (running shoe)\n",
      "100/100 [==============================] - 4s 42ms/step\n",
      "Evaluating normal images: TFR: 0.35, Accuracy: 0.35\n",
      "100/100 [==============================] - 4s 42ms/step\n",
      "Evaluating adversarial images: TFR: 0.35, Accuracy: 0.35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dictionaries used to record results of the evaluation. Each dict has sub-dictionaries for each model and\n",
    "# target label\n",
    "normal_results = dict()\n",
    "adv_results = dict()\n",
    "num_steps_dict = dict()\n",
    "\n",
    "for image_file_name in os.listdir(\"./adv_textures\"):\n",
    "    adv_texture = data.Model3D._get_texture(\"./adv_textures/{}\".format(image_file_name))\n",
    "\n",
    "    # extract information from file name of adversarial texture, inclduing which model and target label is the\n",
    "    # texture for\n",
    "    current_model_name, current_target_label, num_steps = parse_adv_texture_file_name(image_file_name)\n",
    "    save_num_steps(num_steps_dict, num_steps, current_model_name, current_target_label)\n",
    "\n",
    "    # find the model that the adversarial texture was made for\n",
    "    current_model = next(x for x in models if x.name == current_model_name)\n",
    "    # get the normal texture of the model\n",
    "    std_texture = current_model.raw_texture\n",
    "    # load the appropriate model into the renderer\n",
    "    uv_renderer.load_obj(current_model.obj_path)\n",
    "\n",
    "    print(\"Creating evaluation renders for model {}, target label {} ({})\".format(\n",
    "        current_model_name, current_target_label, imagenet_labels[current_target_label]))\n",
    "    std_images, adv_images = render_images_for_texture(std_texture, adv_texture, uv_renderer, current_model_name,\n",
    "                                                       current_target_label)\n",
    "\n",
    "    # evaluate renders with the normal image\n",
    "    predictions = victim_model.predict(std_images, batch_size=1)\n",
    "    accuracy, tfr = get_tfr_and_accuracy(current_model, current_target_label, predictions)\n",
    "    # record results in dictionary and on the command line\n",
    "    save_result(normal_results, tfr, current_model_name, current_target_label, tfr=True)\n",
    "    save_result(normal_results, accuracy, current_model_name, current_target_label, tfr=False)\n",
    "    print(\"Evaluating normal images: TFR: {}, Accuracy: {}\".format(tfr, accuracy))\n",
    "\n",
    "    # evaluate renders with the adversarial image\n",
    "    predictions = victim_model.predict(adv_images, batch_size=1)\n",
    "    accuracy, tfr = get_tfr_and_accuracy(current_model, current_target_label, predictions)\n",
    "    # record results in dictionary and on the command line\n",
    "    save_result(adv_results, tfr, current_model_name, current_target_label, tfr=True)\n",
    "    save_result(adv_results, accuracy, current_model_name, current_target_label, tfr=False)\n",
    "    print(\"Evaluating adversarial images: TFR: {}, Accuracy: {}\\n\".format(tfr, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6fd502-067a-40c8-8cbd-946dbfb287ca",
   "metadata": {},
   "source": [
    "Calculate mean classification accuracy for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9114eaa8-97d6-483b-b5f3-7d22a01c7c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy for normal images: 0.6028\n",
      "Average TFR for normal images: 0.0008\n",
      "Average accuracy for adversarial images: 0.0086\n",
      "Average TFR for adversarial images: 0.8008000000000001\n",
      "\n",
      "\n",
      "Average TFR on adversarial images for model barrel: 0.9559999999999998\n",
      "Average classification accuracy on normal images for model barrel: 0.9280000000000002\n",
      "Average iterations for creating adversarial examples for model barrel: 1428.4\n",
      "\n",
      "Average TFR on adversarial images for model baseball: 0.806\n",
      "Average classification accuracy on normal images for model baseball: 1.0\n",
      "Average iterations for creating adversarial examples for model baseball: 6349.2\n",
      "\n",
      "Average TFR on adversarial images for model camaro: 0.9339999999999999\n",
      "Average classification accuracy on normal images for model camaro: 0.10800000000000001\n",
      "Average iterations for creating adversarial examples for model camaro: 2585.8\n",
      "\n",
      "Average TFR on adversarial images for model clownfish: 0.48200000000000004\n",
      "Average classification accuracy on normal images for model clownfish: 0.372\n",
      "Average iterations for creating adversarial examples for model clownfish: 9999.0\n",
      "\n",
      "Average TFR on adversarial images for model german_shepherd: 0.8560000000000001\n",
      "Average classification accuracy on normal images for model german_shepherd: 0.732\n",
      "Average iterations for creating adversarial examples for model german_shepherd: 5243.0\n",
      "\n",
      "Average TFR on adversarial images for model orange: 0.974\n",
      "Average classification accuracy on normal images for model orange: 0.59\n",
      "Average iterations for creating adversarial examples for model orange: 1163.2\n",
      "\n",
      "Average TFR on adversarial images for model purse: 0.42800000000000005\n",
      "Average classification accuracy on normal images for model purse: 0.7100000000000001\n",
      "Average iterations for creating adversarial examples for model purse: 9090.6\n",
      "\n",
      "Average TFR on adversarial images for model sea_turtle: 0.8039999999999999\n",
      "Average classification accuracy on normal images for model sea_turtle: 0.898\n",
      "Average iterations for creating adversarial examples for model sea_turtle: 5123.0\n",
      "\n",
      "Average TFR on adversarial images for model taxi: 0.8780000000000001\n",
      "Average classification accuracy on normal images for model taxi: 0.15\n",
      "Average iterations for creating adversarial examples for model taxi: 4229.6\n",
      "\n",
      "Average TFR on adversarial images for model teddy: 0.89\n",
      "Average classification accuracy on normal images for model teddy: 0.54\n",
      "Average iterations for creating adversarial examples for model teddy: 2782.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Average accuracy for normal images: {}\".format(get_average_metric(normal_results, for_tfr=False)))\n",
    "print(\"Average TFR for normal images: {}\".format(get_average_metric(normal_results, for_tfr=True)))\n",
    "\n",
    "print(\"Average accuracy for adversarial images: {}\".format(get_average_metric(adv_results, for_tfr=False)))\n",
    "print(\"Average TFR for adversarial images: {}\\n\".format(get_average_metric(adv_results, for_tfr=True)))\n",
    "\n",
    "for model_name in adv_results:\n",
    "    print(\"\\nAverage TFR on adversarial images for model {}: {}\".format(model_name, get_average_metric_for_model(\n",
    "        adv_results, model_name, for_tfr=True)))\n",
    "    print(\"Average classification accuracy on normal images for model {}: {}\".format(\n",
    "        model_name, get_average_metric_for_model(normal_results, model_name, for_tfr=False)))\n",
    "    print(\"Average iterations for creating adversarial examples for model {}: {}\".format(\n",
    "        model_name, get_average_num_steps(num_steps_dict, model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962075c4-d285-41ae-aefc-46d2e69544f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
